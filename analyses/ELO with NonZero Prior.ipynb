{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to build a very simple model to predict the outcome of tennis matched.  This is an ELO model.  Specifically, we will model\n",
    "\n",
    "$$p(y_1) = \\sigma(\\beta_1 - \\beta_2)$$, where $$\\beta_1$$ and $$\\beta_2$$ are coefficients for players 1 and 2 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.logit.base import get_match_result_data\n",
    "\n",
    "df, player_mapping, inverse_player_mapping = get_match_result_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50784, 62)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50k matches to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training data\n",
    "\n",
    "First, we'll just try training on everything in this time period and see what happens.  Later, we will split into train and test sets and figure out appropriate discounting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.logit.base import get_X_y\n",
    "\n",
    "X, y = get_X_y(df, player_mapping, include_ranks=False)\n",
    "assert ((X > 0).sum(axis=1) == 1).all()\n",
    "assert ((X < 0).sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Logistic Regression with NonZero Prior\n",
    "\n",
    "I'll start with a zero prior and go from there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def describe_result(coefs, inverse_player_map):\n",
    "    coef_df = pd.DataFrame(list(enumerate(coefs[:len(inverse_player_map)])),\n",
    "        columns=['player_idx', 'coef']\n",
    "    )\n",
    "    coef_df['player_name'] = coef_df['player_idx'].map(lambda x: inverse_player_map[x])\n",
    "    return coef_df.sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.prior_logit import NonZeroLogit\n",
    "\n",
    "nzl = NonZeroLogit(lmbda=1., seed=10)\n",
    "nzl.fit(X, y)\n",
    "describe_result(nzl.beta, inverse_player_mapping).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(fit_intercept=False)\n",
    "logit.fit(X, y)\n",
    "describe_result(logit.coef_[0], inverse_player_mapping).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Time-Decay and Regularization\n",
    "\n",
    "There is an argument that we should not be regularizing towards 0.  Players that appear infrequently are probably wors -- they qualify for fewer tournaments and progress to later rounds of tournaments less often.  For now however, we will continue to regularize towards 0.  Here, we investigate the following:\n",
    "\n",
    "* What time decay weight is most predictive?\n",
    "* How much regularization should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by updating model every New Year's Day\n",
    "BURN_IN_DATE = 2010\n",
    "END_DATE = 2019\n",
    "TIME_BREAKS = ['%d-01-01' % year for year in range(BURN_IN_DATE, END_DATE)]\n",
    "TIME_BREAKS = map(pd.to_datetime, TIME_BREAKS)\n",
    "TIME_BREAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_model(train_X, train_y, test_X, test_y, weights=None, lmbda=1.0, prior=0.):\n",
    "    print 'Fitting model...'\n",
    "    logit = NonZeroLogit(lmbda=lmbda, prior=prior)\n",
    "    if weights is None:\n",
    "        weights = np.ones(train_X.shape[0])\n",
    "    logit.fit(train_X, train_y, sample_weight=weights)\n",
    "    print 'Predicting probs...'\n",
    "    preds = logit.predict_proba(test_X)[:, 1] \n",
    "    print 'Predicting classes...'\n",
    "    class_preds = logit.predict(test_X)\n",
    "    return logit, preds, class_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that get_weights is doing what it's supposed to be doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.logit.base import _get_weights\n",
    "\n",
    "weights = _get_weights(df['date'].max(), df, 365.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(df['date'], weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the weighting is doing roughly what it is supposed to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval_model(lmbda=1.0, halflife=None, prior=0., max_rank=None, include_ranks=False):\n",
    "    if max_rank is not None:\n",
    "        _df = df[\n",
    "            (df['wrank'] < max_rank) &\n",
    "            (df['lrank'] < max_rank)\n",
    "        ].copy()\n",
    "    else:\n",
    "        _df = df.copy()\n",
    "        \n",
    "    mods = []\n",
    "    out = []\n",
    "    _df['__weight__'] = 10000 *_get_weights(\n",
    "        _df['date'].max(),\n",
    "        _df,\n",
    "        halflife\n",
    "    )\n",
    "    for i in range(len(TIME_BREAKS)):\n",
    "        cur_break = TIME_BREAKS[i]\n",
    "        if i + 1 == len(TIME_BREAKS):\n",
    "            next_break = pd.to_datetime('2099-12-31')\n",
    "        else:\n",
    "            next_break = TIME_BREAKS[i + 1]\n",
    "        print cur_break, next_break\n",
    "        train_df = _df[_df['date'] < cur_break]\n",
    "        test_df = _df[\n",
    "            (_df['date'] >= cur_break) &\n",
    "            (_df['date'] < next_break)\n",
    "        ]\n",
    "        train_X, train_y = get_X_y(train_df, player_mapping, include_ranks=include_ranks)\n",
    "        test_X, test_y = get_X_y(test_df, player_mapping, include_ranks=include_ranks)\n",
    "        mod, preds, class_preds = fit_model(\n",
    "            train_X,\n",
    "            train_y,\n",
    "            test_X,\n",
    "            test_y,\n",
    "            lmbda=lmbda,\n",
    "            weights=train_df['__weight__']\n",
    "        )\n",
    "        mods.append(mod)\n",
    "        out.extend(zip(test_df['match_id'].tolist(), preds.tolist(), class_preds.tolist()))\n",
    "    out_df = pd.DataFrame(out, columns=['match_id', 'pred', 'class_pred'])\n",
    "    final_df = pd.merge(_df, out_df, on='match_id')\n",
    "\n",
    "    if max_rank is not None:\n",
    "        rel = final_df[\n",
    "            (final_df['lrank'] < max_rank) &\n",
    "            (final_df['wrank'] < max_rank)\n",
    "        ]\n",
    "    else:\n",
    "        rel = final_df\n",
    "\n",
    "    auc = roc_auc_score(rel['y'], rel['pred'])\n",
    "    accuracy = (rel['y'] == rel['class_pred']).mean() \n",
    "    print auc, accuracy\n",
    "    return mods, out, final_df, auc, accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ATTEMPTS = 20\n",
    "lmbdas = np.power(10., np.random.uniform(-3, 3, size=N_ATTEMPTS))\n",
    "hls = np.random.uniform(100, 4 * 365, size=N_ATTEMPTS)\n",
    "priors = np.random.uniform(-5., 0., size=N_ATTEMPTS)\n",
    "out = []\n",
    "for lmbda, hl, prior in zip(lmbdas, hls, priors):\n",
    "    print lmbda, hl, prior\n",
    "    _, _, _, auc, accuracy = eval_model(lmbda=lmbda, halflife=hl, prior=prior)\n",
    "    out.append((lmbda, hl, prior, auc, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_df = pd.DataFrame(\n",
    "    out,\n",
    "    columns=[\n",
    "        'lambda',\n",
    "        'halflife',\n",
    "        'prior',\n",
    "        'auc',\n",
    "        'accuracy'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_df.sort('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods, _, final_df, _, _ = eval_model(lmbda=100.0, halflife=365, prior=-2.0, max_rank=None, include_ranks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['tournament', 'winner', 'loser', 'winner_idx', 'loser_idx', 'p1_idx', 'p2_idx', 'pred', 'y']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tb, mod in zip(TIME_BREAKS, mods):\n",
    "    print '\\n' + str(tb)\n",
    "    print '\\n---------------------------'\n",
    "    print describe_result(mod.beta, inverse_player_mapping).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we calibrated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "_x, _y = calibration_curve(final_df['y'], final_df['pred'])\n",
    "plt.plot(_x, _y)\n",
    "plt.plot(_x, _x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pessimistic on the lower end, optimistic on the higher end..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_player(player_name):\n",
    "    return df[\n",
    "        df['winner'].map(lambda x: player_name in x) |\n",
    "        df['loser'].map(lambda x: player_name in x)\n",
    "    ][[\n",
    "        'date',\n",
    "        'winner',\n",
    "        'loser',\n",
    "    ]].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate_player('Roddick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Model that only uses rank to make class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank_pred'] = (df['p1_rank'] < df['p2_rank'])\n",
    "rel_df = df[\n",
    "    (df['date'] > TIME_BREAKS[0]) &\n",
    "    df['p1_rank'].notnull() &\n",
    "    df['p2_rank'].notnull()\n",
    "]\n",
    "(rel_df['rank_pred'] == rel_df['y']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66.6% from using rank alone...but this_updates_constantly and this only looks at matches where both players have rankings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get year end rankings\n",
    "rank_df = df.copy()\n",
    "rank_df['year'] = rank_df['date'].map(lambda x: x.year)\n",
    "_winner_rank_df = rank_df.groupby(['year', 'winner']).apply(\n",
    "    lambda x: x[['year', 'winner', 'date', 'wrank']].iloc[-1]\n",
    ").reset_index(drop=True)\n",
    "_winner_rank_df.rename(columns={'winner': 'player'}, inplace=True)\n",
    "_winner_rank_df.rename(columns={'wrank': 'ye_rank'}, inplace=True)\n",
    "\n",
    "_loser_rank_df = rank_df.groupby(['year', 'loser']).apply(\n",
    "    lambda x: x[['year', 'loser', 'date', 'lrank']].iloc[-1]\n",
    ").reset_index(drop=True)\n",
    "_loser_rank_df.rename(columns={'loser': 'player'}, inplace=True)\n",
    "_loser_rank_df.rename(columns={'lrank': 'ye_rank'}, inplace=True)\n",
    "\n",
    "tot_rank_df = pd.concat([_winner_rank_df, _loser_rank_df])\n",
    "tot_rank_df.sort('date', ascending=True, inplace=True)\n",
    "year_end_rankings = tot_rank_df.drop_duplicates(['player', 'year'], take_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_for_merge = year_end_rankings.copy()\n",
    "_for_merge['year'] += 1  # Want to match year end rankings for 2000 with matches in 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df = pd.merge(\n",
    "    rank_df,\n",
    "    _for_merge,\n",
    "    left_on=['winner', 'year'],\n",
    "    right_on=['player', 'year']\n",
    ")\n",
    "rank_df.rename(columns={'ye_rank': 'winner_ye_rank'}, inplace=True)\n",
    "\n",
    "rank_df = pd.merge(\n",
    "    rank_df,\n",
    "    _for_merge,\n",
    "    left_on=['loser', 'year'],\n",
    "    right_on=['player', 'year']\n",
    ")\n",
    "rank_df.rename(columns={'ye_rank': 'loser_ye_rank'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = rank_df[\n",
    "    (rank_df['date'] > TIME_BREAKS[0]) &\n",
    "    rank_df['winner_ye_rank'].notnull() &\n",
    "    rank_df['loser_ye_rank'].notnull()\n",
    "]\n",
    "(rank_df['winner_ye_rank'] < rank_df['loser_ye_rank']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 64% accuracy when we use year-end rankings so apples-to-apples time comparison and can't update rank all the time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
